<html>
<!-- This is a comment in HTML -->
<!--body background="Results/background.jpg"-->

<head>

<title>Welcome to David Lavy's Computer Vision webpage</title>
<!-- The title tag sets the title bar of the web bar -->

<style>
table, th, td {
    border: 1px solid black;
    border-collapse: collapse;
}
th, td {
    padding: 5px;
    text-align: center;
}
</style>

</head>

<!--body bgcolor="#ffffff"-->
<body background="background2.jpg">
<!-- The <body> tag lets the browser know that we are beginning the body of the web page.  The bgcolor option sets the background color to the web page -->

   <h1>Welcome to David Lavy's Computer Vision webpage</h1>
   <!-- The <h1> tag sets the text to a Heading 1 style.  This is the largest and boldest of all the various heading styles. -->


<h2><b>Programming Assignment 4</b></h2>

<p>CS 585 HW2 <br>
David Lavy <br>
Sean Matuszak, Sean Smith <br>
Wednesday, October 27th
</p>

<hr>

<p style="font-size: 120%;"><b>Problem definition</b></p>

<p>
In previous assignments, we have demonstrated the problems and solutions to identifying and analysing objects, but an important aspect we did not cover was the tracking of those object. In this assignment, we attempted to use various methods to track objects in a video. Using the segmented video, we tracked bats based on their centroids.
</p>
<ul>
    <li>Challenges</li>
    <ul>
        <li>Implementation: Initially we implemented an alpha-beta filter to predict the bats movements, then used a greedy algorithm to assign the locations in the next frame to the tracked bats. This naive approach proved buggy, so we then implemented the more complicated Kalman Filter with the hungarian tracking method.</li>
        <li>Occlusion: Bats fly in front of each other, occlusing the image and temporarily sharing a centroid.</li>
        <li>Entering and exiting bats: Bats fly in and out of the frame, as well as flying in the z-axis so that they become segmented.</li>
        <li>Spurious detection: There are times where there are spurious objects detected by the segemntation algorithm that should not be tracked.</li>
    </ul>
    <!--
    <li>Assumptions</li>
    <ul>
        <li>Aquarium: We will make our system lightning invariant by only using our value channel from the HSV space, and consider small isolated motion pixels as fish, due to the fact that using and erode and dilation step gets rid of a lot of the fish in the images.</li>
        <li>Bats: We will be using the grayscale images and because our algorithm worked fairly well (e.g. no considerable noise) we assumed that all the blobs we got are bats.</li>
        <li>Cells: We made the assumption that when getting our contour of the cells, because of the bright values around them we get stripes that they don't connect, our assumption was that if these stripes are close enough they belong to the same boundary of the cell and therefore by using morphological operations we can get the whole contour and shape of the cell.</li>
    </ul>
    -->
</ul>

<hr>
<h2> Method and Implementation </h2>

<p>We initially implemented the alpha-beta filter to predict the location of the bats, and used a greedy algorthim to assign predictions to found centroids in the next frame. We were able to create a work-around for occlusion; if no centroids were found in a given radius, we allowed the tracked bat to use a centroid already taken previously by the greedy algorithm (because when bats are occluded, they temporarily share a centroid). To adjust for bats exiting the screen, if the projected x or y value was outside the size of the picture, we stopped tracking that centroid. For any centroids not previously tracked by bats, we created a new bat-tracking structure for it. This implementation had limited success, confined to regions where the bats were sufficiently far apart (the top).</p>
<p>The results of greedy/alpha-beta left much to be desired, so we started implementing a more sophisticated method: the Kalman Filter with Hungarian assignments. The Hungarian algorithm is designed to deal with too many/too few bats when the matrix isn't NxN; if there are too few bats, we start tracking the new centroids. If there are too many bats, we first consider occlusion; if the point reappears according to what the Kalman Filter predicts, we jump there. If this does not happen within 10 frames, we consider the bat lost and stop tracking it.
</p>


<hr>
<h2>Experiments</h2>
<p>
Our initial experiment involved tracking the bats with greedy/alpha-beta, followed by tracking the bats with Hungarian/Kalman. We assigned each bat a new random color, and drew the trajectory of the bat in that color.
</p>


<hr>
<h2> Results</h2>

<table style="width:100%">
    <caption><b>Results</b></caption>
    <tr>
        <th><b>Original</b></th>
        <th><b>Greedy Algorithm - Alpha-Beta Filter</b></th>
        <th><b>Hungarian Algorithm - Kalman Filter</b></th>
    </tr>
    <tr>
        <td><div align="center">
<iframe width="420" height="315" src="https://www.youtube.com/embed/kvyncQ2jHPU" frameborder="0" allowfullscreen></iframe>
        </div></td>
        <td><div align="center">
<iframe width="420" height="315" src="https://www.youtube.com/embed/ahcEHqVxHFU" frameborder="0" allowfullscreen></iframe>
        </div></td>
        <td><div align="center">
<iframe width="420" height="315" src="https://www.youtube.com/embed/w9mQGjM5TmI" frameborder="0" allowfullscreen></iframe>
        </div></td>
    </tr>
   </table>

<p>
<hr>
<h2> Discussion </h2>

<p>
The greedy algorthim with an alpha-beta filter had some moderate success. Bats we tracked to a degree, especially in areas where there weren't interfering bats. The issues with this approach are self-evident, and due to the greedy/non-optimal nature of the algorithm. The greedy algorithm is especially in problematic in areas where bats are clustered because if the greedy algorithm takes the wrong centroid early-on, it causes a cascade of wrongly tracked centroids.
</p>
<p>
The Hungarian algorithm with a Kalman filter proved more successful, with much smoother and reliable tracks. It fails to re-track bats after occlusion in at least one case, but is very successful otherwise. One minor problem is the way we trace the bats has a bug where is a bat flies over the path it is erased, but that is not a problem with the algorithm.
</p>
<hr>
<h2> Conclusions </h2>

<p>
Our results demonstrate the pros and cons of various existing methods in object tracking. The greedy algorithm, though much easier to implement, was less effective at assigning the bats. The Hungarian algorithm, partnered with the Kalman filter produced strong results. Although we found it suffiecient to use the Kalman filter with only the past data point, an area for further research would be implementing multiple hypothesis tracking.
</p>


<hr>
<h2> Credits and Bibliography </h2>
<p>
Hungarian algorithm reference: https://github.com/Smorodov/Multitarget-tracker <a href="https://github.com/Smorodov/Multitarget-tracker">  </a>10/26/15
</p>
<p>
OpenCV Kalman filter: http://docs.opencv.org/master/dd/d6a/classcv_1_1KalmanFilter.html#gsc.tab=0 <a href="http://docs.opencv.org/master/dd/d6a/classcv_1_1KalmanFilter.html#gsc.tab=0">  </a>10/26/15
</p>
<p>
Segmented videos provided in the assignment
</p>
<p>
Partners: Sean Smith, Sean Matuszak
</p>

<hr>
<hr>






   <h2><b>Programming Assignment 3</b></h2>

   <p>CS 585 HW2 <br>
      David Lavy <br>
      Sean Matuszak, Sean Smith <br>
      Thursday, October 8th
   </p>

   <hr>

   <p style="font-size: 120%;"><b>Problem definition</b></p>  

<p>
    Detecting and analysing objects is an important part of computer vision. We set out to detect certain objects in videos, detect connected components, lower the number of components, and perform statistics on these components. We did this for 3 sets of videos: an aquarium, cells, and bats.
    </p>
    <ul>
     <li>Challenges</li>
     <ul>
       <li>Aquarium: This set of images was the most challenging one. This is because there exists multiple objects moving (big fish, small fish, seaweed). Also the reflection of the light and the pumped oxigen in the top right corner is deceiving to our algorithm, as we're trying to track the fish, but also the moving seaweed gets tracked. Another thing to notice is that small movement of the fish, and sometimes only just a part of their body moving as well as occlusion  makes the tracking difficult.</li>
       <li>Bats: The bats images weren't too difficult, except that we needed to come up with a good tracking algorithm for the lower part of the image where the brightness is too much that it gets mixed with the grayscale colors of the bats. Our first approach using iterative threshold wasn't as effective as the others as it is shown in our results.</li>
       <li>Cells: The cells images showed some difficult due to the existence of bright pixels around each cell, while having darker image regions. By using a classic threshold approach would either only get boundaries (that aren't continuous) or mixed the inside of the cell with the whole background.</li>
       <li>In terms of code implementation, challenges arised when creating our video, as this was hard in a Windows machine, but fairly easy in a Linux machine. Another challenge was the implementation of our connected components code, due to time constrains we weren't able to develop an efficient piece of code for this (although our algorithm ran fairly well and fast).</li>
     </ul>
     <li>Assumptions</li>
     <ul>
       <li>Aquarium: We will make our system lightning invariant by only using our value channel from the HSV space, and consider small isolated motion pixels as fish, due to the fact that using and erode and dilation step gets rid of a lot of the fish in the images.</li>
       <li>Bats: We will be using the grayscale images and because our algorithm worked fairly well (e.g. no considerable noise) we assumed that all the blobs we got are bats.</li>
       <li>Cells: We made the assumption that when getting our contour of the cells, because of the bright values around them we get stripes that they don't connect, our assumption was that if these stripes are close enough they belong to the same boundary of the cell and therefore by using morphological operations we can get the whole contour and shape of the cell.</li>
     </ul>
     
   </ul>
    
    <hr>
    <h2> Method and Implementation </h2>

<p>For all data sets, we used two segmentation methods: iterative threshold and adaptive threshold. We implemented our own connected components algorithm quasi-recursively using a stack.  After all this we tailored each program to the dataset. For the aquarium, we didn't change the output (see discussion for explanation). For the bat dataset, we eroded to reduce single-pixel objects. For cells, we ran into an interesting problem; cells were being broken into many small components. To fix this, we dialted several times to fuse these components into one object, then eroded to approximate the cell.
For all data sets, statistics were generated for each object in the image and saved to a .csv file at runtime using opencv functions.
</p>
    
    
    <hr>
    <h2>Experiments</h2>
    <p>
    This problem was difficult to run experiments with. Most of our decisions were subjectively determined within the group as to what best approximated our given video.
    </p>
    
    
    <hr>
    <h2> Results</h2>

    <table style="width:100%">
      <caption><b>Table of templates</b></caption>
     <tr>
       <th><b>Original</b></th>
       <th><b>Adaptive Threshold</b></th>
       <th><b>Iterative Threshold</b></th>
     </tr>
     <tr>
       <td><div align="center">
<iframe width="420" height="315" src="https://www.youtube.com/embed/dMMPcqAKGGw" frameborder="0" allowfullscreen></iframe>
 </div></td>
       <td><div align="center">
<iframe width="420" height="315" src="https://www.youtube.com/embed/3mJvyBN0b5k" frameborder="0" allowfullscreen></iframe>
 </div></td>
       <td><div align="center">
<iframe width="420" height="315" src="https://www.youtube.com/embed/ZAAEebXXsN8" frameborder="0" allowfullscreen></iframe>
 </div></td>
     </tr>
     <tr>
       <td><div align="center">
<iframe width="420" height="315" src="https://www.youtube.com/embed/n9pcr9bG8cs" frameborder="0" allowfullscreen></iframe>
 </div></td>
       <td><div align="center">
<iframe width="420" height="315" src="https://www.youtube.com/embed/AwiPaXH-kNw" frameborder="0" allowfullscreen></iframe>
 </div></td>
       <td><div align="center">
<iframe width="420" height="315" src="https://www.youtube.com/embed/K3jvCzhRlKU" frameborder="0" allowfullscreen></iframe>
 </div></a></td>
     </tr>
     <tr>
       <td><div align="center">
<iframe width="420" height="315" src="https://www.youtube.com/embed/Cfyuqb2gTIQ" frameborder="0" allowfullscreen></iframe>
 </div></td>
       <td><div align="center">
<iframe width="420" height="315" src="https://www.youtube.com/embed/-R-VF0qM3n4" frameborder="0" allowfullscreen></iframe>
 </div></td>
       <td><div align="center">
<iframe width="420" height="315" src="https://www.youtube.com/embed/bdpSZiMov4Q" frameborder="0" allowfullscreen></iframe>
 </div></a></td>
     </tr>
   </table>

   <p>
   <table style="width:100%">
      <caption><b>Table of statistics for Adaptive Threshold</b></caption>
     <tr>
       <th><b>Aquarium</b></th>
       <th><b>Bats</b></th>
       <th><b>Cells</b></th>
     </tr>
     <tr>
       <th><table border="1">
<thead><tr><th title="Field #1">Frame Number</th>
<th title="Field #2">Area</th>
<th title="Field #3">Orientation (rad)</th>
<th title="Field #4">Circularity</th>
<th title="Field #5">Perimeter</th>
<th title="Field #6">Compactness</th>
</tr></thead>
<tbody><tr><td>1</td>
<td align="right">3.0</td>
<td align="right">0.1793850</td>
<td>0.490579</td>
<td align="right">6</td>
<td>1.33333</td>
</tr>
<tr><td>1</td>
<td align="right">4.5</td>
<td align="right">-0.0634864</td>
<td>0.0268898</td>
<td align="right">5</td>
<td>1.55556</td>
</tr>
<tr><td>1</td>
<td align="right">2.0</td>
<td align="right">0.6435010</td>
<td>0.242424</td>
<td align="right">6</td>
<td>2</td>
</tr>
<tr><td>1</td>
<td align="right">0.5</td>
<td align="right">2.3561900</td>
<td>0.333333</td>
<td align="right">5</td>
<td>14</td>
</tr>
<tr><td>1</td>
<td align="right">1.0</td>
<td align="right">0.0000000</td>
<td>1</td>
<td align="right">4</td>
<td>6</td>
</tr>
<tr><td>1</td>
<td align="right">4.0</td>
<td align="right">1.9756900</td>
<td>0.246753</td>
<td align="right">7</td>
<td>1.25</td>
</tr>
<tr><td>1</td>
<td align="right">0.5</td>
<td align="right">2.3561900</td>
<td>0.333333</td>
<td align="right">3</td>
<td>2</td>
</tr>
<tr><td>1</td>
<td align="right">17.0</td>
<td align="right">-0.0449429</td>
<td>0.107555</td>
<td align="right">13</td>
<td>0.882353</td>
</tr>
<tr><td>1</td>
<td align="right">6.0</td>
<td align="right">0.0000000</td>
<td>0.333333</td>
<td align="right">6</td>
<td>0.666667</td>
</tr>
<tr><td>1</td>
<td align="right">4.0</td>
<td align="right">-0.2513310</td>
<td>0.10785</td>
<td align="right">8</td>
<td>2.5</td>
</tr>
<tr><td>1</td>
<td align="right">10.5</td>
<td align="right">-0.1289640</td>
<td>0.370486</td>
<td align="right">11</td>
<td>0.857143</td>
</tr>
<tr><td>1</td>
<td align="right">0.0</td>
<td align="right">0.0000000</td>
<td>0</td>
<td align="right">2</td>
<td> -nan</td>
</tr>
<tr><td>1</td>
<td align="right">0.0</td>
<td align="right">0.0000000</td>
<td>0</td>
<td align="right">2</td>
<td> -nan</td>
</tr>
<tr><td>1</td>
<td align="right">2.0</td>
<td align="right">0.6172840</td>
<td>0.00901812</td>
<td align="right">11</td>
<td>4.5</td>
</tr>
<tr><td>1</td>
<td align="right">8.0</td>
<td align="right">0.3505460</td>
<td>0.130445</td>
<td align="right">7</td>
<td>0.625</td>
</tr>
<tr><td>1</td>
<td align="right">4.0</td>
<td align="right">0.0000000</td>
<td>0.0625</td>
<td align="right">4</td>
<td>1.5</td>
</tr>
<tr><td>1</td>
<td align="right">0.0</td>
<td align="right">0.0000000</td>
<td> inf</td>
<td align="right">1</td>
<td> inf</td>
</tr>
<tr><td>1</td>
<td align="right">15.0</td>
<td align="right">0.9407350</td>
<td>0.324684</td>
<td align="right">13</td>
<td>1</td>
</tr>
</tbody></table></th>
       <th><table border="1">
<thead><tr><th title="Field #1">Frame Number</th>
<th title="Field #2">Area</th>
<th title="Field #3">Orientation (rad)</th>
<th title="Field #4">Circularity</th>
<th title="Field #5">Perimeter</th>
<th title="Field #6">Compactness</th>
</tr></thead>
<tbody><tr><td>1</td>
<td align="right">2.0</td>
<td align="right">1.570800</td>
<td> 0.25</td>
<td align="right">4</td>
<td> 3</td>
</tr>
<tr><td>1</td>
<td align="right">4.5</td>
<td align="right">-0.270210</td>
<td> 0.489206</td>
<td align="right">6</td>
<td> 0.888889</td>
</tr>
<tr><td>1</td>
<td align="right">2.0</td>
<td align="right">0.000000</td>
<td> 0.25</td>
<td align="right">4</td>
<td> 3</td>
</tr>
<tr><td>1</td>
<td align="right">1.0</td>
<td align="right">1.570800</td>
<td> 0.333333</td>
<td align="right">3</td>
<td> 1</td>
</tr>
<tr><td>1</td>
<td align="right">8.0</td>
<td align="right">1.994430</td>
<td> 0.211487</td>
<td align="right">10</td>
<td> 1</td>
</tr>
<tr><td>1</td>
<td align="right">7.0</td>
<td align="right">1.216480</td>
<td> 0.304323</td>
<td align="right">9</td>
<td> 1.57143</td>
</tr>
<tr><td>1</td>
<td align="right">11.0</td>
<td align="right">1.773240</td>
<td> 0.281097</td>
<td align="right">9</td>
<td> 1</td>
</tr>
<tr><td>1</td>
<td align="right">26.0</td>
<td align="right">1.144410</td>
<td> 0.313441</td>
<td align="right">9</td>
<td> 0.423077</td>
</tr>
<tr><td>1</td>
<td align="right">6.0</td>
<td align="right">-0.231824</td>
<td> 0.0878283</td>
<td align="right">8</td>
<td> 1.66667</td>
</tr>
<tr><td>1</td>
<td align="right">9.5</td>
<td align="right">0.154026</td>
<td> 0.591698</td>
<td align="right">7</td>
<td> 0.526316</td>
</tr>
<tr><td>1</td>
<td align="right">23.5</td>
<td align="right">-0.480466</td>
<td> 0.510342</td>
<td align="right">8</td>
<td> 0.425532</td>
</tr>
<tr><td>1</td>
<td align="right">5.0</td>
<td align="right">0.314398</td>
<td> 0.330001</td>
<td align="right">6</td>
<td> 0.8</td>
</tr>
<tr><td>1</td>
<td align="right">2.5</td>
<td align="right">0.515188</td>
<td> 0.489206</td>
<td align="right">5</td>
<td> 2.8</td>
</tr>
<tr><td>1</td>
<td align="right">80.5</td>
<td align="right">1.750830</td>
<td> 0.139071</td>
<td align="right">37</td>
<td> 0.484472</td>
</tr>
<tr><td>1</td>
<td align="right">0.0</td>
<td align="right">0.000000</td>
<td> inf</td>
<td align="right">1</td>
<td> inf</td>
</tr>
<tr><td>1</td>
<td align="right">0.0</td>
<td align="right">0.000000</td>
<td> 0</td>
<td align="right">2</td>
<td> -nan</td>
</tr>
<tr><td>1</td>
<td align="right">7.0</td>
<td align="right">-0.548255</td>
<td> 0.0764445</td>
<td align="right">6</td>
<td> 0.571429</td>
</tr>
<tr><td>1</td>
<td align="right">32.5</td>
<td align="right">1.837870</td>
<td> 0.174926</td>
<td align="right">14</td>
<td> 0.369231</td>
</tr>
</tbody></table></th>
       <th><table border="1">
<thead><tr><th title="Field #1">Frame Number</th>
<th title="Field #2">Area</th>
<th title="Field #3">Orientation (rad)</th>
<th title="Field #4">Circularity</th>
<th title="Field #5">Perimeter</th>
<th title="Field #6">Compactness</th>
</tr></thead>
<tbody><tr><td align="right">1</td>
<td align="right">1504.5</td>
<td align="right">0.6032960</td>
<td align="right">0.4788380</td>
<td align="right">35</td>
<td align="right">0.0219342</td>
</tr>
<tr><td align="right">1</td>
<td align="right">6155.5</td>
<td align="right">1.7842100</td>
<td align="right">0.0237071</td>
<td align="right">223</td>
<td align="right">0.0359029</td>
</tr>
<tr><td align="right">1</td>
<td align="right">48.5</td>
<td align="right">0.7853980</td>
<td align="right">0.8620690</td>
<td align="right">5</td>
<td align="right">0.1443300</td>
</tr>
<tr><td align="right">1</td>
<td align="right">2568.0</td>
<td align="right">0.6149250</td>
<td align="right">0.2233270</td>
<td align="right">85</td>
<td align="right">0.0338785</td>
</tr>
<tr><td align="right">1</td>
<td align="right">1007.5</td>
<td align="right">0.5212410</td>
<td align="right">0.1131830</td>
<td align="right">63</td>
<td align="right">0.0605459</td>
</tr>
<tr><td align="right">1</td>
<td align="right">190.5</td>
<td align="right">-0.1139780</td>
<td align="right">0.2945030</td>
<td align="right">11</td>
<td align="right">0.0472441</td>
</tr>
<tr><td align="right">1</td>
<td align="right">1867.5</td>
<td align="right">0.0317736</td>
<td align="right">0.3489530</td>
<td align="right">54</td>
<td align="right">0.0278447</td>
</tr>
<tr><td align="right">1</td>
<td align="right">2143.5</td>
<td align="right">1.1892200</td>
<td align="right">0.2528400</td>
<td align="right">46</td>
<td align="right">0.0205272</td>
</tr>
<tr><td align="right">1</td>
<td align="right">2054.5</td>
<td align="right">1.4270800</td>
<td align="right">0.7261220</td>
<td align="right">60</td>
<td align="right">0.0301777</td>
</tr>
<tr><td align="right">2</td>
<td align="right">1114.5</td>
<td align="right">1.8805900</td>
<td align="right">0.6426750</td>
<td align="right">32</td>
<td align="right">0.0305070</td>
</tr>
<tr><td align="right">2</td>
<td align="right">2209.5</td>
<td align="right">1.7678900</td>
<td align="right">0.1507810</td>
<td align="right">92</td>
<td align="right">0.0425436</td>
</tr>
<tr><td align="right">2</td>
<td align="right">1446.0</td>
<td align="right">2.2378800</td>
<td align="right">0.8417280</td>
<td align="right">46</td>
<td align="right">0.0304288</td>
</tr>
<tr><td align="right">2</td>
<td align="right">2121.5</td>
<td align="right">0.6120910</td>
<td align="right">0.3741100</td>
<td align="right">69</td>
<td align="right">0.0334669</td>
</tr>
<tr><td align="right">2</td>
<td align="right">584.5</td>
<td align="right">1.4759100</td>
<td align="right">0.2002900</td>
<td align="right">42</td>
<td align="right">0.0684346</td>
</tr>
<tr><td align="right">2</td>
<td align="right">1733.0</td>
<td align="right">0.2365360</td>
<td align="right">0.3589690</td>
<td align="right">52</td>
<td align="right">0.0311598</td>
</tr>
<tr><td align="right">2</td>
<td align="right">240.5</td>
<td align="right">1.6591200</td>
<td align="right">0.0459334</td>
<td align="right">10</td>
<td align="right">0.0332640</td>
</tr>
<tr><td align="right">2</td>
<td align="right">1105.5</td>
<td align="right">1.0045600</td>
<td align="right">0.6039940</td>
<td align="right">34</td>
<td align="right">0.0289462</td>
</tr>
<tr><td align="right">2</td>
<td align="right">2529.5</td>
<td align="right">-0.0170933</td>
<td align="right">0.1816260</td>
<td align="right">60</td>
<td align="right">0.0245108</td>
</tr>
</tbody></table></th>
     </tr>
   </table>

        <hr>
    <h2> Discussion </h2>
    
    <p>
    Our methods proved very successful for Bats and Cells. The aquarium data set proved challenging due to the diversity of objects to be identified, the small size of objects, and many moving objects in the background. It was interesting that segmentation algorithm coices are project-dependant.
    </p>
    <p>
An interesting observation is that we created a "rainbow" effect when labelling components. Although we tried to conserve color, if the algorithm detected a different object first, they would switch color. For data sets with a large number of components or fast-moving objects, colors would appear to be strobing.
    </p>
    <p>
    A significant problem we encountered occurred with the aquarium data set. The results contained many small fish and lots of noise. We tried to reduce the noise by eroding then dilating, but this left us with boxes instead of fish. Fish would be eroded down to a single pixel, and the squares that resulted from dilating did not approximate the fish well. The small size of the fish and the sheer number of fish proved a problem.
    </p>
    <p>
    Another problem came from using iterative threshold on the cell dataset. There was not much color variation, so the iterative threshold did not give us a good result.
    </p>
    <hr>
    <h2> Conclusions </h2>
    
    <p>
    We had strong results, leading to the conclusion that segmentation methods of iterative and adaptive thresholding are highly useful, with the caveat that iterative thresholding should not be used on data sets that are close-to-uniformly-colored. The weaknesses were that these methods aren't necessarily robust; for each dataset, we needed to tailor a solution to "fix" the data. Eroding and dilating were very useful, except in the aquarium dataset. An improvement would be to find a way to lower the number of fish in the aquarium dataset.
    </p>
    
    
    <hr>
    <h2> Credits and Bibliography </h2>
  <!--
  <p>
    
    OpenCV template matching: <a href="http://docs.opencv.org/doc/tutorials/imgproc/histograms/template_matching/template_matching.html">http://docs.opencv.org/doc/tutorials/imgproc/his
        tograms/template_matching/template_matching.html</a> 9/23/15
     
    </p>
   -->
  <p>
  OpenCV statistics: http://docs.opencv.org/modules/imgproc/doc/structural_analysis_and_shape_descriptors.html?highlight=findcontours#findcontours <a href="http://docs.opencv.org/modules/imgproc/doc/structural_analysis_and_shape_descriptors.html?highlight=findcontours#findcontours">  </a>10/7/15
  </p>
  <p>
  Segmentation methods and connected components algorithm taught in-class by Margrit Betke
  </p>
    <p>
    Partners: Sean Smith, Sean Matuszak
    </p>

   <hr>
   <hr>

   <h2><b>Programming Assignment 2</b></h2>
   <!-- The <p> tag denotes a new paragraph -->

   <p>CS 585 HW2 <br>
      David Lavy <br>
      Elham Saraee, Andria Ellis <br>
      Thursday, September 24
   </p>

   <hr>

   <p style="font-size: 120%;"><b>Problem definition</b></p>

   <p>Track an object in a video. This will be performed by using a picture of the isolated object and use it as a template. Then using a webcam the user will hold the object and the program will draw a bounding box where the template matching algorithm finds the object.
   <ul>
     <li>Challenges</li>
     <ul>
       <li>Scale variance: When the object is too far or too close the template matching will not work correctly as it will try to find a small pen in a big box.</li>
       <li>Rotation invariance: Template will have only one direction. If it is asymmetric, rotating the object in the video will cause errors in the matching</li>
       <li>Good video quality and illumination: These two factors can add a lot of noise to the camera. This happened to me using a Linux machine. We made sure we had mostly daylight and also good working cameras.</li>
       <li>Understand what's happening under the hood: As OpenCV uses a 'blackbox' to calculate the matrix of coeff. using the matchingtemplate method, we corroborated our results by doing an approximation of the values and make sure these were normalized in the range of [0, 1]</li>
     </ul>
     <li>Assumptions</li>
     <ul>
       <li>For this part, we assume that there was no rotation invariance, nor scale invariance.</li>
       <li>As we're using Normalized Cross-Correlation, lightning and exposure aren't a great deal. So as long as the template and the camera video are visible, the tracking works just fine.</li>
       <li>The object to track in this case is a rectangle.</li>
     </ul>
     
   </ul>

   <hr>

   <p style="font-size: 120%;"><b>Method and Implementation</b></p>

   <p>The method we used here is simple. A call of the matchTemplate function in OpenCV. To do this first we stream our webcam (or a video previously recorded) together with a template we will like to use. In our experiment we took a picture holding a card and then crop it. This will be our template for tracking. Then we call our function MatchingMethod and show the rectangle where our template is located. The function is described below.

   <p>For this task we created one function called MatchingMethod, what this does is performing the template matching using match_method=CV_TM_SQDIFF_NORMED which is the one discussed in class. Once this was calculated we looked in our image where was the highest coeffcient located and draw a rectangle in it.

   <hr>

   <p style="font-size: 120%;"><b>Experiments</b></p>
   <p>For this first part we took the image of a Charlie card. We did this procedure offline and online. As this part was relatively simple the tracking was done just once. But is important to highlight that we tried this setting in 3 different locations at 3 rooms illuminated differently.</p>

   <p>Metrics
     <ul>
       <li>Detection rate: This was about 90%. There were certain times in the matching that the rectangle went off the true matching. This was mostly caused by rapid movements and also rotation of the card which made it hard to track.</li>
       <li>Running time: For the calculation of detection described above we used a video of length of 15 seconds.</li>   
     </ul>

   <hr>

   <p style="font-size: 120%;"><b>Results</b></p>   

    <table style="width:100%">
     <caption><b>Results</b></caption>
     <tr>
       <th><b>Method</b></th>
       <th><b>Original Image</b></th>
       <th><b>Result Image</b></th>
     </tr>
     <tr>
       <td><b>Matching Method using NCC (Online)</b></td>
       <td><a href="http://imgur.com/Jchk1b1"><img src="http://imgur.com/Jchk1b1.png" width="480" height="360" title="Offline image" /></a></td>
       <td><a href="http://imgur.com/BIlrGEU"><img src="http://imgur.com/BIlrGEU.png" width="480" height="360" title="Template Matching Offline" /></a>
     </tr>
     <tr>
       <td><b>Matching Method using NCC (Offline)</b></td>
       <td><a href=""><img src="http://imgur.com/QfHSVaR.png" width="480" height="360" title="Online Image (webcam)" /></a></td>
       <td><a href="http://imgur.com/LeAEvfZ"><img src="http://imgur.com/LeAEvfZ.png" width="480" height="360" title="Template Matching Online" /></a>
     </tr>
   </table>

   <hr>

   <p style="font-size: 120%;"><b>Discussion</b></p>

   <p>
     <ul>
       <li>An advantage of our method is the use of NCC which makes our system invariant to lightning and exposure.</li>
       <li>The weakness is that we aren't accounting for scale and rotation invariance, which makes a different angle for our template make our system 50% of the time find another matching rectangle.</li>
       <li>We can improve our method by using a set of downsampled images to account for the scale invariance. Also for rotation we can find edges of our stream and verify the direction of potential matchings and rotate them in the same position as our template</li>
     </ul>
   </p>
   
   <hr>
     
   <p style="font-size: 120%;"><b>Conclusions</b></p>
   <p>This was a good introduction to template-based matching algorithms. Future experiments will involve try different algorithms and verify which one is more efficient.</p>
   
   <hr>

   <p style="font-size: 120%;"><b>Bibliography</b></p>
   <p> - http://docs.opencv.org/doc/tutorials/imgproc/histograms/template_matching/template_matching.html

   <hr>

 <p style="font-size: 120%;"><b>Part 2</b></p>
   <p style="font-size: 120%;"><b>Problem definition</b></p>

       <p>Track an our hand in the video. Also detect 3 shapes and output which is the shape we are making and delineate the shape of the hand. 
       <p>We are addressing 3 different challenges here. 
       <p>First we need to perform a skin detection algorithm to isolate only the skin in our video. Then we will do a template matching with this output together with our template, which has been processed with the skin detection as well so we only have skin as a grayscale image and the background will be black. After this we will do 3 different template matching and look at their coefficients. The highest of these 3 coefficients will tell us which shape is the one we are making. Finally, we will copy the code we did in part one to create a rectangle in the matching and using our skin detection image we will delineate the shape and put this contour back in the original stream.
   <ul>
     <li>Challenges</li>
     <ul>
       <li>Scale variance: Same as in our part 1, scale invariance is a problem we need to address, however we found out that the hand detection will work well in a certain range and it also detects the shape.</li>
       <li>Rotation invariance: This is a very difficult problem, that we could not address, shapes of the hand are more difficult to find orientation. We read some papers about how to do this that will be mentioned at the end.</li>
       <li>Background and noise supression: When there is a 'lot of skin' in the background this impacted our system with low performance.</li>
       <li>Rapid movement of the hand. Usually in our system we have our face and our hands. If the hands move rapidly we will lose track of them, and we found out that the system goes for the face as a potential match. This was improved if we moved our hand slowly.</li>
     </ul>
     <li>Assumptions</li>
     <ul>
       <li>We assumed that our skin detection varies according to lightning and exposure, this is because when using the same parameters in different environments, there are big blobs in different images. To address this we created a calibration system that will give us the parameters we need for skin detection.</li>
       <li>Also we tried to mantain a uniform background so it is easier to isolate the sking with our algorithm</li>
       <li>The object to track in this case will be bounded in a rectangle.</li>
     </ul>
     
   </ul>

   <hr>

   <p style="font-size: 120%;"><b>Method and Implementation</b></p>

   <p>The method we used here is as we described above. We first take the HSV parameters for skin detection. Later we start streaming images from the webcam, transform these into HSV and apply our skin detection function.This will give us a grayscale image where the grayscale part will only be applied to our skin. We do the same for our templates. 
   <p>Once this is done we apply our MatchingMethod described in part one, but with a little modification that will be decribed below. This function will give us the coefficient of the template matching which will be compared between the three of these matchings and finally the one that's the biggest will determine what shape is being used.
   <p>Finally we will use the rectangle from our MatchingMethod function and extract the contour of this which will be our hand shape. 

   <p>For this task we created a separate function called skinHSV which we can perform a manual skin detection calibration by determining the HSV min and max parameters that later will be used in our main program. 
   <p>In our main program we have the function mySkinDetect that was done in class, but this was modified for our method using HSV parameters. And it will output a grayscale image.
   <p>My MatchingMethod function has been modified by outputting the matching rectangle where we will draw our contour and this contour later will be drawn in the streaming image. Also it outputs the maximum coefficient of the matrix given by the matchTemplate OpenCV function.
   <p>Finally a function called myMaxD will give us the maximum coefficient of the three we got from doing MatchingMethod.

   <hr>

   <p style="font-size: 120%;"><b>Experiments</b></p>
   <p>For this second part we took many different templates, we tried our algorithm with three main shapes which are: Open Hand, Peace sign and Spock ('live long and prosper'). However it is important to point that we used other templates as well (Right point, left point, up point, fist, etc). Then we carried our experiments in many different scenarios, with people in it, dark background, light background. Also we performed many different offline and online experiments. Specially in my case, my light bulbs at home are not really powerful and this created a very dark image that was really difficult to perform my skin detection. </p>

   <p>Metrics
     <ul>
       <li>Detection rate: This was about 67%. The tracking of the first two shapes worked pretty good, but the third one we had problems because the matching was not done properly. After debugging our algorithm we could not find what was the cause of this problem.</li>
       <li>Running time: This running time was more extense and we used videos of length about 45 - 60 seconds trying different shapes over and over again.</li>
     </ul>   

   <hr>

   <p style="font-size: 120%;"><b>Results</b></p>   

    <table style="width:100%">
      <caption><b>Table of templates</b></caption>
     <tr>
       <th><b>Template 1</b></th>
       <th><b>Template 2</b></th>
       <th><b>Template 3</b></th>
     </tr>
     <tr>
       <td><a href="http://i.imgur.com/92E2jjN"><img src="http://i.imgur.com/92E2jjN.png" width="240" height="320" title="Template1" /></a></td>
       <td><a href="http://i.imgur.com/pOaXgPT"><img src="http://i.imgur.com/pOaXgPT.png" width="240" height="320" title="Template2" /></a></td>
       <td><a href="http://i.imgur.com/Ouq50Ji"><img src="http://i.imgur.com/Ouq50Ji.png" width="240" height="320" title="Template3" /></a>
     </tr>
     <tr>
       <td><a href="http://i.imgur.com/S9Z1MS5"><img src="http://i.imgur.com/S9Z1MS5.png" width="480" height="360" title="Matching template 1" /></a></td>
       <td><a href="http://i.imgur.com/xyHFAXy"><img src="http://i.imgur.com/xyHFAXy.png" width="480" height="360" title="Matching template 2" /></a></td>
       <td><a href="http://i.imgur.com/u2bjDJi"><img src="http://i.imgur.com/u2bjDJi.png" width="480" height="360" title="Matching Template 3" /></a>
     </tr>
   </table>

   <hr>

   <p style="font-size: 120%;"><b>Discussion</b></p>

   <p>
     <ul>
       <li>The strength of our project is how we addressed our template matching. We  think that doing a previous skin detection isolates other features that are irrelevant and this will make the matching easier.</li>
       <li>Another strength is our program that calculates the HSV parameters in each situation, this is helpful because we can try in almost any situation, and we can be sure that skin detection will be optimized.
       <li>The weakness is that are not accounting for scale and rotation invariance. A slight move of for example our open hand makes it seem like it is a peace sign, this is confusing.</li>
       <li>We can improve our method by using a set of downsampled images to account for the scale invariance. For rotation invariance we can use 3D cameras that will also capture depth, this will give us extra information and the shapes will not be mixed as we saw what happened in our algorithm.</li>
     </ul>
   </p>

       <p>Confussion Matrix (Done for the first 3 seconds of each shape):
       <p>Done to a total of 270 frames:
       <p>
<style type="text/css">
.tg  {border-collapse:collapse;border-spacing:0;}
.tg td{font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;}
.tg th{font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;}
.tg .tg-oopb{color:#cb0000}
.tg .tg-wvtg{background-color:#34bdc9}
.tg .tg-jhgd{background-color:#3186af}
.tg .tg-rpj7{background-color:#ad6864}
.tg .tg-698h{background-color:#000000}
</style>
<table class="tg">
  <tr>
    <th class="tg-031e">Results\Data</th>
    <th class="tg-031e">Open Hand</th>
    <th class="tg-031e">Peace Sign</th>
    <th class="tg-031e">Spock</th>
    <th class="tg-rpj7">Overall Classifications</th>
    <th class="tg-wvtg">Precision</th>
  </tr>
  <tr>
    <td class="tg-031e">Open Hand</td>
    <td class="tg-oopb">85</td>
    <td class="tg-oopb">4</td>
    <td class="tg-oopb">1</td>
    <td class="tg-rpj7">21</td>
    <td class="tg-wvtg">94%</td>
  </tr>
  <tr>
    <td class="tg-031e">Peace Sign</td>
    <td class="tg-oopb">6</td>
    <td class="tg-oopb">80</td>
    <td class="tg-oopb">4</td>
    <td class="tg-rpj7">90</td>
    <td class="tg-wvtg">89%</td>
  </tr>
  <tr>
    <td class="tg-031e">Spock</td>
    <td class="tg-oopb">18</td>
    <td class="tg-oopb">63</td>
    <td class="tg-oopb">9</td>
    <td class="tg-rpj7">90</td>
    <td class="tg-wvtg">10%</td>
  </tr>
  <tr>
    <td class="tg-jhgd">Recall</td>
    <td class="tg-jhgd">78%</td>
    <td class="tg-jhgd">54%</td>
    <td class="tg-jhgd">64%</td>
    <td class="tg-698h" colspan="2"></td>
  </tr>
</table>
   
   <hr>
     
   <p style="font-size: 120%;"><b>Conclusions</b></p>
   <p>This was a very challenging project. Many different methods were tried, a lot of errors were made, but overall it was good learning in how we can do shape detection. I hope to improve this version and make it work with the 3D points I mentioned above using a Kinect.</p>
   
   <hr>

   <p style="font-size: 120%;"><b>Bibliography</b></p>
   <p> - http://docs.opencv.org/doc/tutorials/imgproc/histograms/template_matching/template_matching.html

   <hr>
   <hr>

   <h2><b>Programming Assignment 1</b></h2>
   <!-- The <p> tag denotes a new paragraph -->

   <p>CS 585 HW1 <br>
      David Lavy <br>
      Thursday, September 10
   </p>

   <hr>

   <p style="font-size: 120%;"><b>Problem definition</b></p>
   <p>Add your original face image and the processed image(s) and a short explanation about your processing algorithm.

   <hr>

   <p style="font-size: 120%;"><b>Experiments</b></p>
   <p>Original Image
   <p>This picture I took it on my first trip to Japan in Kyoto. One of the best places to visit.

   <p>Experiment 1 - Grayscale Image
   <p>I used the OpenCV function cvtColor with argument RGB2GRAY, this uses the algorithm to convert the pixel: x = 0.21r + 0.72g + 0.07b. If you need more information about this visit this <a
 href="http://docs.opencv.org/doc/user_guide/ug_mat.html#primitive-operations">link</a>

   <p>Experiment 2 - Canny Edge Detector
   <p>Canny Edge Detector acts by smoothing the image using a Gaussian filter and then find the gradients of the image, and using a min and max threshold it determines potential edges. This is a very popular edge detector method. And I used the function Canny from OpenCV. A simple tutorial can be found <a href="http://docs.opencv.org/doc/tutorials/imgproc/imgtrans/canny_detector/canny_detector.html">here</a> 

   <hr>

   <p style="font-size: 120%;"><b>Method and Implementation</b></p>
   <p>Add your original face image and the processed image(s) and a short explanation about your processing algorithm.

   <hr>

   <p style="font-size: 120%;"><b>Results</b></p>

   <table style="width:100%">
     <caption><b>Results</b></caption>
     <tr>
       <th><b>Method</b></th>
       <th><b>Original Image</b></th>
       <th><b>Result Image</b></th>
     </tr>
     <tr>
       <td><b>Grayscale</b></td>
       <td><a href="http://imgur.com/1FQEfrS"><img src="http://i.imgur.com/1FQEfrS.png" width="480" height="360" title="Original Image" /></a></td>
       <td><a href="http://imgur.com/yRi4mzU"><img src="http://i.imgur.com/yRi4mzU.png" width="480" height="360" title="Grayscale Image" /></a>
     </tr>
     <tr>
       <td><b>Canny Edge Detector</b></td>
       <td><a href="http://imgur.com/1FQEfrS"><img src="http://i.imgur.com/1FQEfrS.png" width="480" height="360" title="Original Image" /></a></td>
       <td><a href="http://imgur.com/laFfmIO"><img src="http://i.imgur.com/laFfmIO.png" width="480" height="360" title="Canny Edge Image" /></a>
     </tr>
   </table>

   <hr>

   <p style="font-size: 120%;"><b>Conclusions</b></p>
   <p>This first lab gave us an intro on how to use OpenCV, learn the syntax, remember some C++ stuff and get ready for lab 2 which will be more tedious.

   <hr>
   
</body>
</html>
